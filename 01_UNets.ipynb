{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k-a8GRH8DXKk"
   },
   "source": [
    "<center><a href=\"https://www.nvidia.com/en-us/training/\"><img src=\"https://dli-lms.s3.amazonaws.com/assets/general/DLI_Header_White.png\" width=\"400\" height=\"186\" /></a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cTQNPPpeDaOk"
   },
   "source": [
    "# 1. From U-Net to Diffusion\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "81PfOMFwKBZG"
   },
   "source": [
    "U-Nets are a type of convolutional neural network that was originally designed for medical imaging. For instance, we could feed the network an image of a heart and it could return a different picture highlighting potentially cancerous areas.\n",
    "\n",
    "Can we use this process to generate new images? Here's an idea: what if we add noise to our images, and then use a U-Net to separate the images from the noise. Could we then feed the model noise and have it create a recognizable image? Let's give it a try!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Pkc4vj3csv3"
   },
   "source": [
    "#### Learning Objectives\n",
    "\n",
    "The goals of this notebook are to:\n",
    "* Explore the FashionMNIST dataset\n",
    "* Build a U-Net architecture\n",
    "  * Construct a Down Block\n",
    "  * Construct an Up Block\n",
    "* Train a model to remove noise from an image\n",
    "* Attempt to generate articles of clothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 364
    },
    "executionInfo": {
     "elapsed": 6260,
     "status": "error",
     "timestamp": 1689940390619,
     "user": {
      "displayName": "Devesh Khandelwal US",
      "userId": "16954520040589783180"
     },
     "user_tz": 420
    },
    "id": "NNmWEhrB-uSm",
    "outputId": "6450de47-6418-4268-82b0-da2f8e3b9fe0"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "\n",
    "# Visualization tools\n",
    "import graphviz\n",
    "from torchview import draw_graph\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AAAgjE5K_ZFZ"
   },
   "source": [
    "In PyTorch, we can use our GPU in our operations by setting the [device](https://pytorch.org/docs/stable/tensor_attributes.html#torch.device) to `cuda`. The function `torch.cuda.is_available()` will confirm PyTorch can recognize the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1689675950620,
     "user": {
      "displayName": "Devesh Khandelwal US",
      "userId": "16954520040589783180"
     },
     "user_tz": 420
    },
    "id": "Dw5___FcYtBQ",
    "outputId": "f418f7d6-cf0c-4b23-87ad-140cc2b1501c"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1689675950620,
     "user": {
      "displayName": "Devesh Khandelwal US",
      "userId": "16954520040589783180"
     },
     "user_tz": 420
    },
    "id": "dfE6Lkmq_Tt4",
    "outputId": "b346ac80-72b6-4db0-e76b-f8cff013d312"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EzHZsOWLCdmB"
   },
   "source": [
    "## 1.1 The Dataset\n",
    "\n",
    "To get practice with generating images, we will use the [FashionMNIST](https://github.com/zalandoresearch/fashion-mnist) dataset. FashionMNIST is designed to be a \"Hello World\" dataset for image classification problems. The small size of the black and white images (28 x 28 pixels) also makes it a great starting point for image generation.\n",
    "\n",
    "FashionMNIST is included in [Torchvision](https://pytorch.org/vision/stable/index.html), a computer vision library associated with PyTorch. When we download the dataset, we can pass a list of [transformations](https://pytorch.org/vision/stable/transforms.html) we would like to apply to the images. For now, we will use [ToTensor](https://pytorch.org/vision/stable/generated/torchvision.transforms.ToTensor.html#torchvision.transforms.ToTensor) to convert the images into a tensors so we can process the images with a neural network. This will automatically scale the pixel values from [0, 255] to [0, 1]. It will also rearrange the dimensions to be from [Height x Width x Channels] to [Channels x Height x Width]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GIRaYbZkDKQa"
   },
   "outputs": [],
   "source": [
    "train_set = torchvision.datasets.FashionMNIST(\n",
    "    \"./data/\", download=True, transform=transforms.Compose([transforms.ToTensor()])\n",
    ")\n",
    "NUM_CLASSES = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can sample some of the images with the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 141
    },
    "executionInfo": {
     "elapsed": 2859,
     "status": "ok",
     "timestamp": 1689675953471,
     "user": {
      "displayName": "Devesh Khandelwal US",
      "userId": "16954520040589783180"
     },
     "user_tz": 420
    },
    "id": "0ICXTYpQDyjv",
    "outputId": "bcb0235c-6d89-473d-997a-128ebc1ef0bb"
   },
   "outputs": [],
   "source": [
    "# Adjust for display; high w/h ratio recommended\n",
    "plt.figure(figsize=(16, 1))\n",
    "\n",
    "def show_images(dataset, num_samples=10):\n",
    "    for i, img in enumerate(dataset):\n",
    "        if i == num_samples:\n",
    "            return\n",
    "        plt.subplot(1, num_samples, i + 1)\n",
    "        plt.imshow(torch.squeeze(img[0]))\n",
    "\n",
    "show_images(train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z8KfDZm7EqXA"
   },
   "source": [
    "Let's set up some import constants for our dataset. With U-Nets, it is common to continually halve the size of the feature map through [Max Pooling](https://paperswithcode.com/method/max-pooling). Then, the feature map size is doubled with [Transposed Convolution](https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d.html). To keep the image dimensions consistent as we move down and up the U-Net, it helps if the image size can be divisible by `2` multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XIU9PzSJEdTp"
   },
   "outputs": [],
   "source": [
    "IMG_SIZE = 16 # Due to stride and pooling, must be divisible by 2 multiple times\n",
    "IMG_CH = 1 # Black and white image, no color channels\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have defined the target size of our images, let's create a function to load the data and transform it to the target size. The random noise we will be adding to our images will be sampled from a [standard normal distribution](https://mathworld.wolfram.com/NormalDistribution.html), meaning 68% of the noise pixel values will be from -1 to 1. We will similarly scale our image values to be from -1 to 1.\n",
    "\n",
    "This would also be a good place to apply random image augmentation. For now, we will start with a [RandomHorizontalFlip](https://pytorch.org/vision/stable/generated/torchvision.transforms.RandomHorizontalFlip.html#torchvision.transforms.RandomHorizontalFlip). We won't use a [RandomVericalFlip](https://pytorch.org/vision/stable/generated/torchvision.transforms.RandomVerticalFlip.html#torchvision.transforms.RandomVerticalFlip) because we would end up generating upside down images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JDeNBgkgGnKE"
   },
   "outputs": [],
   "source": [
    "def load_fashionMNIST(data_transform, train=True):\n",
    "    return torchvision.datasets.FashionMNIST(\n",
    "        \"./\",\n",
    "        download=True,\n",
    "        train=train,\n",
    "        transform=data_transform,\n",
    "    )\n",
    "\n",
    "\n",
    "def load_transformed_fashionMNIST():\n",
    "    data_transforms = [\n",
    "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "        transforms.ToTensor(),  # Scales data into [0,1]\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.Lambda(lambda t: (t * 2) - 1)  # Scale between [-1, 1]\n",
    "    ]\n",
    "\n",
    "    data_transform = transforms.Compose(data_transforms)\n",
    "    train_set = load_fashionMNIST(data_transform, train=True)\n",
    "    test_set = load_fashionMNIST(data_transform, train=False)\n",
    "    return torch.utils.data.ConcatDataset([train_set, test_set])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jNR_1j7iHYqt"
   },
   "outputs": [],
   "source": [
    "data = load_transformed_fashionMNIST()\n",
    "dataloader = DataLoader(data, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vb_BfRbWEPyR"
   },
   "source": [
    "## 1.2 The U-Net Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cjn5q_dtEe01"
   },
   "source": [
    "First, let's define the different components of our U-Net architecture. Primarily, the `DownBlock` and the `UpBlock`.\n",
    "\n",
    "### 1.2.1 The Down Block\n",
    "\n",
    "The `DownBlock` is a typical convolutional neural network. If you are new to PyTorch and are coming from a Keras/TensorFlow background, the below is more similar to the [functional API](https://keras.io/guides/functional_api/) instead of a [sequential model](https://keras.io/guides/sequential_model/). We will later be using [residual](https://stats.stackexchange.com/questions/321054/what-are-residual-connections-in-rnns) and skip connections. A sequential model does not have the flexibility to support these types of connections, but a functional model does.\n",
    "\n",
    "In our `__init__` function below, we will assign our various neural network operations to class variables:\n",
    "* [Conv2d](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html) applies convolution to the input. The `in_ch` is the number of channels we are convolving over and `out_ch` is the number of output channels, which is the same as the number of kernel filters used for convolution. Typically in a U-Net architecture, the number of channels increase the further down we move in the model.\n",
    "* [ReLU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html) is the activation function for the convolution kernels.\n",
    "* [BatchNorm2d](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html) applies [batch normalization](https://towardsdatascience.com/batch-normalization-in-3-levels-of-understanding-14c2da90a338) to a layer of neurons. ReLu has no learnable parameters, so we can apply the same function to multiple layers and it will have the same effect as using multiple ReLu functions. Batch Normalization does have learnable parameters, and reusing this function can have unexpected effects.\n",
    "* [MaxPool2D](https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html) is what we'll use to reduce the size of our feature map as it moves down the network. It's possible to achieve this effect through convolution, but max pooling is commonly used with U-Nets.\n",
    "\n",
    "In the `forward` method, we describe how are various functions should be applied to an input. So far, the operations are sequential in this order:\n",
    "* `Conv2d`\n",
    "* `BatchNorm2d`\n",
    "* `ReLU`\n",
    "* `Conv2d`\n",
    "* `BatchNorm2d`\n",
    "* `ReLU`\n",
    "* `MaxPool2d`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zEV5ZSK0Hj6l"
   },
   "outputs": [],
   "source": [
    "class DownBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        kernel_size = 3\n",
    "        stride = 1\n",
    "        padding = 1\n",
    "\n",
    "        super().__init__()\n",
    "        layers = [\n",
    "            nn.Conv2d(in_ch, out_ch, kernel_size, stride, padding),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_ch, out_ch, kernel_size, stride, padding),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        ]\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 The Up Block\n",
    "\n",
    "While the `DownBlock` reduces the size of our feature map, the `UpBlock` doubles it back up. This is accomplished with [ConvTranspose2d](https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d.html). We can use almost the same architecture as the `DownBlock`, but we will replace conv2 with convT. The tranpose's `stride` of 2 will cause the doubling with the right amount of `padding`.\n",
    "\n",
    "Let's get some practice with the code block below. We've set up an example to test out this function by creating a test image of `1`s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch, h, w = 1, 3, 3\n",
    "x = torch.ones(1, ch, h, w)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the identity `kernel` to see how `conv_transpose2d` alters the input image. The identity kernel has a single `1` value. When used to convolve, the output will be the same as the input.\n",
    "\n",
    "Try changing the `stride`, `padding`, and `output_padding` below. Do the results match your expectation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = torch.tensor([[1.]])  # Identity kernel\n",
    "kernel = kernel.view(1, 1, 1, 1).repeat(1, ch, 1, 1) # Make into a batch\n",
    "\n",
    "output = F.conv_transpose2d(x, kernel, stride=1, padding=0, output_padding=0)[0]\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The kernel size also impacts the size of the output feature map. Try changing `kernel_size` below. Notice how the output image expands as the kernel size increases? This is the opposite of regular convolution, where a larger kernel size decreases the size of the output feature map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_size = 3\n",
    "kernel = torch.ones(1, 1, kernel_size, kernel_size)\n",
    "\n",
    "output = F.conv_transpose2d(x, kernel, stride=1, padding=0, output_padding=0)[0]\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eHtzWNIBLdnX"
   },
   "source": [
    "Another interesting difference: we will be multiplying the input channel by 2. This is to accommodate the skip connections. We will be concatenating the output of an `UpBlock`'s matching `DownBlock` with the `UpBlock`'s input.\n",
    "\n",
    "<center><img src=\"images/FMUNet.png\" width=\"600\" /></center>\n",
    "\n",
    "If x is the size of the input feature map, the output size is:\n",
    "\n",
    "`new_x = (x - 1) * stride + kernel_size - 2 * padding + out_padding`\n",
    "\n",
    "If stride = 2 and out_padding = 1, then in order to double the size of the input feature map:\n",
    "\n",
    "`kernel_size = 2 * padding + 1`\n",
    "\n",
    "The operations are almost the same as before, but with two differences:\n",
    "* `ConvTranspose2d` - Convolution Transpose instead of Convolution\n",
    "* `BatchNorm2d`\n",
    "* `ReLU`\n",
    "* `Conv2d`\n",
    "* `BatchNorm2d`\n",
    "* `ReLU`\n",
    "* ~~`MaxPool2d`~~ - Scaling up instead of down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KV5lmZ4pH22N"
   },
   "outputs": [],
   "source": [
    "class UpBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        # Convolution variables\n",
    "        kernel_size = 3\n",
    "        stride = 1\n",
    "        padding = 1\n",
    "\n",
    "        # Transpose variables\n",
    "        strideT = 2\n",
    "        out_paddingT = 1\n",
    "\n",
    "        super().__init__()\n",
    "        # 2 * in_chs for concatednated skip connection\n",
    "        layers = [\n",
    "            nn.ConvTranspose2d(2 * in_ch, out_ch, kernel_size, strideT, padding, out_paddingT),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_ch, out_ch, kernel_size, stride, padding),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU()\n",
    "        ]\n",
    "        self.model = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x, skip):\n",
    "        x = torch.cat((x, skip), 1)\n",
    "        x = self.model(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M3NXiknvZYjm"
   },
   "source": [
    "### 1.2.3 A Full U-Net\n",
    "\n",
    "It's finally time to piece it together! Below, we have our full `UNet` model.\n",
    "\n",
    "In the `__init__` function, we can define the number of channels at each step of the U-Net with `down_chs`. The current default is `(16, 32, 64)` meaning the current dimensions of the data as it moves through the model are:\n",
    "\n",
    "* input: 1 x 16 x 16\n",
    "* down0: 16 x 16 x 16\n",
    "  * down1: 32 x 8 x 8\n",
    "    * down2: 64 x 4 x 4\n",
    "      * dense_emb: 1024\n",
    "    * up0: 64 x 4 x 4\n",
    "  * up1: 64 x 8 x 8\n",
    "* up2: 32 x 16 x 16\n",
    "* out: 1 x 16 x 16\n",
    "\n",
    "The `forward` class method is where we will finally add our skip connections. For each step down in the U-Net, we will keep track of the output of each `DownBlock`. Then, when we move through the `UpBlock`s, we will [concatenate](https://pytorch.org/docs/stable/generated/torch.cat.html) the output of the previous `UpBlock` with its corresponding `DownBlock`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2884,
     "status": "ok",
     "timestamp": 1688418501290,
     "user": {
      "displayName": "Danielle Detering US",
      "userId": "15432464718872067879"
     },
     "user_tz": 420
    },
    "id": "QS0SCfWPI0DY",
    "outputId": "40929df8-9702-4610-bc52-cb2fe119df53"
   },
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        img_ch = IMG_CH\n",
    "        down_chs = (16, 32, 64)\n",
    "        up_chs = down_chs[::-1]  # Reverse of the down channels\n",
    "        latent_image_size = IMG_SIZE // 4 # 2 ** (len(down_chs) - 1)\n",
    "\n",
    "        # Inital convolution\n",
    "        self.down0 = nn.Sequential(\n",
    "            nn.Conv2d(img_ch, down_chs[0], 3, padding=1),\n",
    "            nn.BatchNorm2d(down_chs[0]),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Downsample\n",
    "        self.down1 = DownBlock(down_chs[0], down_chs[1])\n",
    "        self.down2 = DownBlock(down_chs[1], down_chs[2])\n",
    "        self.to_vec = nn.Sequential(nn.Flatten(), nn.ReLU())\n",
    "        \n",
    "        # Embeddings\n",
    "        self.dense_emb = nn.Sequential(\n",
    "            nn.Linear(down_chs[2]*latent_image_size**2, down_chs[1]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(down_chs[1], down_chs[1]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(down_chs[1], down_chs[2]*latent_image_size**2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Upsample\n",
    "        self.up0 = nn.Sequential(\n",
    "            nn.Unflatten(1, (up_chs[0], latent_image_size, latent_image_size)),\n",
    "            nn.Conv2d(up_chs[0], up_chs[0], 3, padding=1),\n",
    "            nn.BatchNorm2d(up_chs[0]),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.up1 = UpBlock(up_chs[0], up_chs[1])\n",
    "        self.up2 = UpBlock(up_chs[1], up_chs[2])\n",
    "\n",
    "        # Match output channels\n",
    "        self.out = nn.Sequential(\n",
    "            nn.Conv2d(up_chs[-1], up_chs[-1], 3, 1, 1),\n",
    "            nn.BatchNorm2d(up_chs[-1]),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(up_chs[-1], img_ch, 3, 1, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        down0 = self.down0(x)\n",
    "        down1 = self.down1(down0)\n",
    "        down2 = self.down2(down1)\n",
    "        latent_vec = self.to_vec(down2)\n",
    "\n",
    "        up0 = self.up0(latent_vec)\n",
    "        up1 = self.up1(up0, down2)\n",
    "        up2 = self.up2(up1, down1)\n",
    "        return self.out(up2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet()\n",
    "print(\"Num params: \", sum(p.numel() for p in model.parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sxj6aHoqY1EH"
   },
   "source": [
    "Let's verify the model architecture with [torchview](https://github.com/mert-kurttutan/torchview). If we have three `down_chs`, there should be two `DownBlock`s, one for each transition. Similarly, there should be two `UpBlock`s. We should also check that there is one skip connection. The \"bottom\" of the U-Net does not need a skip connection, so there is a skip connection for each `UpBlock` minus one.\n",
    "\n",
    "Finally, are the output dimensions the same as the input dimensions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 539,
     "status": "ok",
     "timestamp": 1688418501827,
     "user": {
      "displayName": "Danielle Detering US",
      "userId": "15432464718872067879"
     },
     "user_tz": 420
    },
    "id": "vL3tWakdYzgl",
    "outputId": "9482d43b-2c2a-46f1-ee5c-f04c489fe114"
   },
   "outputs": [],
   "source": [
    "graphviz.set_jupyter_format('png')\n",
    "model_graph = draw_graph(\n",
    "    model,\n",
    "    input_size=(BATCH_SIZE, IMG_CH, IMG_SIZE, IMG_SIZE),\n",
    "    device='meta',\n",
    "    expand_nested=True\n",
    ")\n",
    "model_graph.resize_graph(scale=1.5)\n",
    "model_graph.visual_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "brm8m3XQJDIS"
   },
   "source": [
    "In [PyTorch 2.0](https://pytorch.org/get-started/pytorch-2.0/), we can compile the model to make training faster. It will send the list of operations to our GPU so it can apply those operations to our inputs much like an assembly line. Read more about it [here](https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ta63x_ZyI3o9"
   },
   "outputs": [],
   "source": [
    "model = torch.compile(UNet().to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "chH-4PKtc0Sh"
   },
   "source": [
    "## 1.3 Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qFecpW0bn9CR"
   },
   "source": [
    "Let's try adding noise to our images and see if our U-Net model can filter it out. We can define a parameter `beta` to represent what percentage of our image is noise versus the original image. We can use `alpha` to represent [the compliment](https://brilliant.org/wiki/probability-by-complement/) to `beta`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n2R5dSecdtVU"
   },
   "outputs": [],
   "source": [
    "def add_noise(imgs):\n",
    "    dev = imgs.device\n",
    "    percent = .5 # Try changing from 0 to 1\n",
    "    beta = torch.tensor(percent, device=dev)\n",
    "    alpha = torch.tensor(1 - percent, device=dev)\n",
    "    noise = torch.randn_like(imgs)\n",
    "    return alpha * imgs + beta * noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "algaplyFowBa"
   },
   "source": [
    "Next, we'll define our loss function as the [Mean Squared Error](https://developers.google.com/machine-learning/glossary#mean-squared-error-mse) between the original image and the predicted image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ued4xgWIcSwU"
   },
   "outputs": [],
   "source": [
    "def get_loss(model, imgs):\n",
    "    imgs_noisy = add_noise(imgs)\n",
    "    imgs_pred = model(imgs_noisy)\n",
    "    return F.mse_loss(imgs, imgs_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iY7Rm5UqTxzH"
   },
   "source": [
    "In order to display the output of our model, we will need to convert it back to the format of an image on the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M9X8l-xqiVTg"
   },
   "outputs": [],
   "source": [
    "def show_tensor_image(image):\n",
    "    reverse_transforms = transforms.Compose([\n",
    "        transforms.Lambda(lambda t: (t + 1) / 2),\n",
    "        transforms.Lambda(lambda t: torch.minimum(torch.tensor([1]), t)),\n",
    "        transforms.Lambda(lambda t: torch.maximum(torch.tensor([0]), t)),\n",
    "        transforms.ToPILImage(),\n",
    "    ])\n",
    "    plt.imshow(reverse_transforms(image[0].detach().cpu()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3L_YXMiHWjYZ"
   },
   "source": [
    "To see the improvement while training, we can compare the `Original` image, the `Noise Added` image, and the `Predicted Original` image using [subplots](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.subplot.html).\n",
    "\n",
    "[@torch.no_grad](https://pytorch.org/docs/stable/generated/torch.no_grad.html) will skip using this function for calculating the gradient during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HkYV78lGg7UH"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def plot_sample(imgs):\n",
    "    # Take first image of batch\n",
    "    imgs = imgs[[0], :, :, :]\n",
    "    imgs_noisy = add_noise(imgs[[0], :, :, :])\n",
    "    imgs_pred = model(imgs_noisy)\n",
    "\n",
    "    nrows = 1\n",
    "    ncols = 3\n",
    "    samples = {\n",
    "        \"Original\" : imgs,\n",
    "        \"Noise Added\" : imgs_noisy,\n",
    "        \"Predicted Original\" : imgs_pred\n",
    "    }\n",
    "    for i, (title, img) in enumerate(samples.items()):\n",
    "        ax = plt.subplot(nrows, ncols, i+1)\n",
    "        ax.set_title(title)\n",
    "        show_tensor_image(img)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZsNiGofWXedR"
   },
   "source": [
    "Finally, it's the moment of truth! It's time to train our model and watch it improve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 58363,
     "status": "ok",
     "timestamp": 1688418560355,
     "user": {
      "displayName": "Danielle Detering US",
      "userId": "15432464718872067879"
     },
     "user_tz": 420
    },
    "id": "bfx4UOBOdOuY",
    "outputId": "9e653657-3f0d-4d29-aa68-8e205b493a68"
   },
   "outputs": [],
   "source": [
    "optimizer = Adam(model.parameters(), lr=0.0001)\n",
    "epochs = 2\n",
    "\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    for step, batch in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        images = batch[0].to(device)\n",
    "        loss = get_loss(model, images)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch % 1 == 0 and step % 100 == 0:\n",
    "            print(f\"Epoch {epoch} | Step {step:03d} Loss: {loss.item()} \")\n",
    "            plot_sample(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qmTLtFc6X25h"
   },
   "source": [
    "There's a little bit of noise in the predicted image, but it still does a decent job of extracting the original clothing.\n",
    "\n",
    "Now, how does the model do when given pure noise? Can it create believable new images?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 3436,
     "status": "ok",
     "timestamp": 1688418563782,
     "user": {
      "displayName": "Danielle Detering US",
      "userId": "15432464718872067879"
     },
     "user_tz": 420
    },
    "id": "-Y_m7Ku9vLym",
    "outputId": "c85c9995-809f-4cd2-95fd-5f97948dc1a4"
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "for _ in range(10):\n",
    "    noise = torch.randn((1, IMG_CH, IMG_SIZE, IMG_SIZE), device=device)\n",
    "    result = model(noise)\n",
    "    nrows = 1\n",
    "    ncols = 2\n",
    "    samples = {\n",
    "        \"Noise\" : noise,\n",
    "        \"Generated Image\" : result\n",
    "    }\n",
    "    for i, (title, img) in enumerate(samples.items()):\n",
    "        ax = plt.subplot(nrows, ncols, i+1)\n",
    "        ax.set_title(title)\n",
    "        show_tensor_image(img)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm, these images look more like ink blot images than clothing. In the next notebook, we will improve upon this technique to create more recognizable images.\n",
    "\n",
    "Before moving on, please restart the jupyter kernel by running the code cell below. This will prevent memory issues in future notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><a href=\"https://www.nvidia.com/en-us/training/\"><img src=\"https://dli-lms.s3.amazonaws.com/assets/general/DLI_Header_White.png\" width=\"400\" height=\"186\" /></a></center>"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
