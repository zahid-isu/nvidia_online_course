{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K2-Px6LAIoz7"
   },
   "source": [
    "<center><a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. CLIP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contrastive Language-Image Pre-Training or [CLIP](https://github.com/openai/CLIP/tree/main) is a text and image encoding tool used with many popular Generative AI models such as [DALL-E](https://openai.com/dall-e-2) and [Stable Diffusion](https://github.com/Stability-AI/stablediffusion).\n",
    "\n",
    "CLIP in itself is not a Generative AI model, but is instead used to align text encodings with image encodings. If there is such a thing as the perfect text description of an image, the goal of CLIP is to create the same vector embedding for both the image and the text. Let's see what this means in practice.\n",
    "\n",
    "The goals of this notebook are to:\n",
    "* Learn how to use CLIP Encodings\n",
    "  * Get an image encoding\n",
    "  * Get a text encoding\n",
    "  * Calculate the cosine similarity between them\n",
    "* Use CLIP to create a text-to-image neural network\n",
    "\n",
    "## 5.1 Encodings\n",
    "\n",
    "First, let's load the libraries needed for this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MWn2WgPaIoz8"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import glob\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Visualization tools\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from torchvision.utils import save_image, make_grid\n",
    "from textwrap import wrap\n",
    "\n",
    "# User defined libraries\n",
    "from utils import other_utils\n",
    "from utils import ddpm_utils\n",
    "from utils import UNet_utils\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few different variations of CLIP based on popular image recognition neural networks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "\n",
    "clip.available_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this notebook, we will be using `ViT-B/32`, which is based on the [Vision Transformer](https://huggingface.co/docs/transformers/main/model_doc/vit) architecture. It has `512` features, which we will later feed into our diffusion model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_model, clip_preprocess = clip.load(\"ViT-B/32\")\n",
    "clip_model.eval()\n",
    "CLIP_FEATURES = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.1 Image Encodings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we load CLIP, it will also come with a set of image transformations we can use to feed images into the CLIP model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can test this on one of our flower photos. Let's start with a picturesque daisy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"data/cropped_flowers/\"\n",
    "img_path = DATA_DIR + \"daisy/2877860110_a842f8b14a_m.jpg\"\n",
    "img = Image.open(img_path)\n",
    "img.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can find the CLIP embedding by first transforming our image with `clip_preprocess` and converting the result to a tensor. Since the `clip_model` expects a batch of images, we can use [np.stack](https://numpy.org/doc/stable/reference/generated/numpy.stack.html) to turn the processed image into a single element batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_imgs = torch.tensor(np.stack([clip_preprocess(img)])).to(device)\n",
    "clip_imgs.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can pass the batch to `clip_model.encode_image` to find the embedding for the image. Uncomment `clip_img_encoding` if you would like to see what an encoding looks like. When we print the size, it lists `512` features for our `1` image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_img_encoding = clip_model.encode_image(clip_imgs)\n",
    "print(clip_img_encoding.size())\n",
    "#clip_img_encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.2 Text Encodings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have an image encoding, let's see if we can get a matching text encoding. Below is a list of different flower descriptions. Like with the images, the text needs to be preprocessed before it can be encoded by CLIP. To do this, CLIP comes with a `tokenize` function in order to convert each word into an integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list = [\n",
    "    \"A white daisy with a yellow center\",\n",
    "    \"An orange sunflower with a big brown center\",\n",
    "    \"A red rose bud\"\n",
    "]\n",
    "text_tokens = clip.tokenize(text_list).to(device)\n",
    "text_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can pass the tokens to `encode_text` to get our text encodings. Uncomment `clip_text_encodings` if you would like to see what an encoding looks like. Similar to our image encoding, there are `512` features for each of our `3` images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_text_encodings = clip_model.encode_text(text_tokens).float()\n",
    "print(clip_text_encodings.size())\n",
    "#clip_text_encodings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.3 Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to see which one of our text descriptions best describes the daisy, we can calculate the [cosine similarity](https://medium.com/@milana.shxanukova15/cosine-distance-and-cosine-similarity-a5da0e4d9ded) between the text encodings and the image encodings. When the cosine similarity is `1`, it's a perfect match. When the cosine similarity is `-1`, the two encodings are opposites.\n",
    "\n",
    "The cosine similarity is equivalent to a [dot product](https://mathworld.wolfram.com/DotProduct.html) with each vector normalized by their magnitude. In other words, the magnitude of each vector becomes `1`.\n",
    "\n",
    "We can use the following formula to calculate the dot product:\n",
    "\n",
    "$X \\cdot Y = \\sum_{i=1}^{n} x_i y_i = x_1y_1 + x_2 y_2 + \\cdots  + x_n y_n$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_img_encoding /= clip_img_encoding.norm(dim=-1, keepdim=True)\n",
    "clip_text_encodings /= clip_text_encodings.norm(dim=-1, keepdim=True)\n",
    "similarity = (clip_text_encodings * clip_img_encoding).sum(-1)\n",
    "similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you think? Does the most descriptive text get the highest score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, text in enumerate(text_list):\n",
    "    print(text, \" - \", similarity[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's practice a little more. Below, we've added a sunflower and a rose image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_paths = [\n",
    "    DATA_DIR + \"daisy/2877860110_a842f8b14a_m.jpg\",\n",
    "    DATA_DIR + \"sunflowers/2721638730_34a9b7a78b.jpg\",\n",
    "    DATA_DIR + \"roses/8032328803_30afac8b07_m.jpg\"\n",
    "]\n",
    "\n",
    "imgs = [Image.open(path) for path in img_paths]\n",
    "for img in imgs:\n",
    "    img.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: The below `get_img_encodings` function is riddled with `FIXMEs`. Please replace each `FIXME` with the appropriate code to generate CLIP encodings from PIL images.\n",
    "\n",
    "Click the `...` for an answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_img_encodings(imgs):\n",
    "    processed_imgs = [FIXME(img) for img in imgs]\n",
    "    clip_imgs = torch.tensor(np.stack(FIXME)).to(device)\n",
    "    clip_img_encodings = FIXME.encode_image(clip_imgs)\n",
    "    return clip_img_encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_img_encodings(imgs):\n",
    "    processed_imgs = [clip_preprocess(img) for img in imgs]\n",
    "    clip_imgs = torch.tensor(np.stack(processed_imgs)).to(device)\n",
    "    clip_img_encodings = clip_model.encode_image(clip_imgs)\n",
    "    return clip_img_encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_img_encodings = get_img_encodings(imgs)\n",
    "clip_img_encodings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Find text that describes the above images well and will result in a high similarity score. After calculating the similarity score, feel free to repeat this exercise and modify. We will be using this text list again later.\n",
    "\n",
    "Click the `...` for an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list = [\n",
    "    \"A daisy\",\n",
    "    \"A sunflower\",\n",
    "    \"A rose\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "```python\n",
    "text_list = [\n",
    "    \"A white daisy with a yellow center\",\n",
    "    \"An orange sunflower with a big brown center\",\n",
    "    \"A deep red rose flower\"\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_tokens = clip.tokenize(text_list).to(device)\n",
    "clip_text_encodings = clip_model.encode_text(text_tokens).float()\n",
    "clip_text_encodings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would be nice to compare each combination of text and image. To do so, we can [repeat](https://pytorch.org/docs/stable/generated/torch.Tensor.repeat.html#torch.Tensor.repeat) each text encoding for each image encoding. Similarly, we can [repeat_interleave](https://pytorch.org/docs/stable/generated/torch.repeat_interleave.html) each image encoding for each text encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_img_encodings /= clip_img_encodings.norm(dim=-1, keepdim=True)\n",
    "clip_text_encodings /= clip_text_encodings.norm(dim=-1, keepdim=True)\n",
    "\n",
    "n_imgs = len(imgs)\n",
    "n_text = len(text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repeated_clip_text_encodings = clip_text_encodings.repeat(n_imgs, 1)\n",
    "repeated_clip_text_encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repeated_clip_img_encoding = clip_img_encodings.repeat_interleave(n_text, dim=0)\n",
    "repeated_clip_img_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity = (repeated_clip_text_encodings * repeated_clip_img_encoding).sum(-1)\n",
    "similarity = torch.unflatten(similarity, 0, (n_text, n_imgs))\n",
    "similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare. Ideally, the diagonal from the top left to the bottom right should be a bright yellow corresponding to their high value. The rest of the values should be low and blue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 10))\n",
    "gs = fig.add_gridspec(2, 3, wspace=.1, hspace=0)\n",
    "\n",
    "for i, img in enumerate(imgs):\n",
    "    ax = fig.add_subplot(gs[0, i])\n",
    "    ax.axis(\"off\")\n",
    "    plt.imshow(img)\n",
    "\n",
    "ax = fig.add_subplot(gs[1, :])\n",
    "plt.imshow(similarity.detach().cpu().numpy().T, vmin=0.1, vmax=0.3)\n",
    "\n",
    "labels = [ '\\n'.join(wrap(text, 20)) for text in text_list ]\n",
    "plt.yticks(range(n_text), labels, fontsize=10)\n",
    "plt.xticks([])\n",
    "\n",
    "for x in range(similarity.shape[1]):\n",
    "    for y in range(similarity.shape[0]):\n",
    "        plt.text(x, y, f\"{similarity[x, y]:.2f}\", ha=\"center\", va=\"center\", size=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 A CLIP Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous notebook, we used the flower category as the label. This time, we're going to use CLIP encodings as our label.\n",
    "\n",
    "If the goal of CLIP is to align text encodings with image encodings, do we need a text description for each of the images in our dataset? Hypothesis: we do not need text descriptions and only need the image CLIP encodings to create a text-to-image pipeline.\n",
    "\n",
    "To test this out, let's add the CLIP encodings as the \"label\" to our dataset. Running CLIP on each batch of data augmented images would be more accurate, but it is also slower. We can speed things up by preprocessing and storing the encodings ahead of time.\n",
    "\n",
    "We can use [glob](https://docs.python.org/3/library/glob.html) to list all of our image filepaths:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_paths = glob.glob(DATA_DIR + '*/*.jpg', recursive=True)\n",
    "data_paths[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next code block runs the following loop for each filepath:\n",
    "* Open the image associated with the path and store it in `img`\n",
    "* Preprocess the image, find the CLIP encoding, and store it in `clip_img`\n",
    "* Convert the CLIP encoding from a tensor to a python list\n",
    "* Store the filepath and the CLIP encoding as a row in a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = 'clip.csv'\n",
    "\n",
    "with open(csv_path, 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile, delimiter=',')\n",
    "    for idx, path in enumerate(data_paths):\n",
    "        img = Image.open(path)\n",
    "        clip_img = torch.tensor(np.stack([clip_preprocess(img)])).to(device)\n",
    "        label = clip_model.encode_image(clip_img)[0].tolist()\n",
    "        writer.writerow([path] + label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It may take a few seconds to process the full dataset. When complete, open [clip.csv](clip.csv) to see the results.\n",
    "\n",
    "We can use the same image transformations as we did with the other notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 32 # Due to stride and pooling, must be divisible by 2 multiple times\n",
    "IMG_CH = 3\n",
    "BATCH_SIZE = 128\n",
    "INPUT_SIZE = (IMG_CH, IMG_SIZE, IMG_SIZE)\n",
    "\n",
    "pre_transforms = [\n",
    "    transforms.Resize(IMG_SIZE),\n",
    "    transforms.ToTensor(),  # Scales data into [0,1]\n",
    "    transforms.Lambda(lambda t: (t * 2) - 1)  # Scale between [-1, 1]\n",
    "]\n",
    "pre_transforms = transforms.Compose(pre_transforms)\n",
    "random_transforms = [\n",
    "    transforms.RandomCrop(IMG_SIZE),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "]\n",
    "random_transforms = transforms.Compose(random_transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the code to initialize our new dataset. Since we've `preprocessed_clip`, we will preload it onto our GPU with the `__init__` function. We've kept the \"on the fly\" CLIP encoding as an example. It will produce slightly better results, but it is much slower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, csv_path, preprocessed_clip=True):\n",
    "        self.imgs = []\n",
    "        self.preprocessed_clip = preprocessed_clip\n",
    "        if preprocessed_clip:\n",
    "            self.labels = torch.empty(\n",
    "                len(data_paths), CLIP_FEATURES, dtype=torch.float, device=device\n",
    "            )\n",
    "        \n",
    "        with open(csv_path, newline='') as csvfile:\n",
    "            reader = csv.reader(csvfile, delimiter=',')\n",
    "            for idx, row in enumerate(reader):\n",
    "                img = Image.open(row[0])\n",
    "                self.imgs.append(pre_transforms(img).to(device))\n",
    "                if preprocessed_clip:\n",
    "                    label = [float(x) for x in row[1:]]\n",
    "                    self.labels[idx, :] = torch.FloatTensor(label).to(device)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = random_transforms(self.imgs[idx])\n",
    "        if self.preprocessed_clip:\n",
    "            label = self.labels[idx]\n",
    "        else:\n",
    "            batch_img = img[None, :, :, :]\n",
    "            encoded_imgs = clip_model.encode_image(clip_preprocess(batch_img))\n",
    "            label = encoded_imgs.to(device).float()[0]\n",
    "        return img, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = MyDataset(csv_path)\n",
    "dataloader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The U-Net model is the same architecture as last time, but with one small difference. Instead of using the number of classes as our `c_embed_dim`, we will use the number of `CLIP_FEATURES`. Last time, `c` might have stood for \"class\", but this time, it stands for \"context\". Thankfully, they both start with `c`, so we do not need to refactor the code to reflect this change in intention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 400\n",
    "B_start = 0.0001\n",
    "B_end = 0.02\n",
    "B = torch.linspace(B_start, B_end, T).to(device)\n",
    "\n",
    "ddpm = ddpm_utils.DDPM(B, device)\n",
    "model = UNet_utils.UNet(\n",
    "    T, IMG_CH, IMG_SIZE, down_chs=(256, 256, 512), t_embed_dim=8, c_embed_dim=CLIP_FEATURES\n",
    ")\n",
    "print(\"Num params: \", sum(p.numel() for p in model.parameters()))\n",
    "model_flowers = torch.compile(model.to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `get_context_mask` function will change a little bit. Since we're replacing our categorical input with a CLIP embedding, we no longer need to one-hot encode our label. We'll still randomly set values in our encoding to `0` to help the model learn without context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context_mask(c, drop_prob):\n",
    "    c_mask = torch.bernoulli(torch.ones_like(c).float() - drop_prob).to(device)\n",
    "    return c_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also recreate the `sample_flowers` function. This time, it will take our `text_list` as a parameter and convert it to a CLIP encoding. The `sample_w` function remains mostly the same and has been moved to the bottom of [ddpm_utils.py](utils/ddpm_utils.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_flowers(text_list):\n",
    "    text_tokens = clip.tokenize(text_list).to(device)\n",
    "    c = clip_model.encode_text(text_tokens).float()\n",
    "    x_gen, x_gen_store = ddpm_utils.sample_w(model, ddpm, INPUT_SIZE, T, c, device)\n",
    "    return x_gen, x_gen_store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to get training! After about `50` `epochs`, the model will start generating something recognizable, and at `100` it will hit its stride. What do you think? Do the generated images match your descriptions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=100\n",
    "c_drop_prob = 0.1\n",
    "lrate = 1e-4\n",
    "save_dir = \"05_images/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=lrate)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    for step, batch in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        t = torch.randint(0, T, (BATCH_SIZE,), device=device).float()\n",
    "        x, c = batch\n",
    "        c_mask = get_context_mask(c, c_drop_prob)\n",
    "        loss = ddpm.get_loss(model_flowers, x, t, c, c_mask)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch} | Step {step:03d} | Loss: {loss.item()}\")\n",
    "    if epoch % 5 == 0 or epoch == int(epochs - 1):\n",
    "        x_gen, x_gen_store = sample_flowers(text_list)\n",
    "        grid = make_grid(x_gen.cpu(), nrow=len(text_list))\n",
    "        save_image(grid, save_dir + f\"image_ep{epoch:02}.png\")\n",
    "        print(\"saved images in \" + save_dir + f\" for episode {epoch}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model is trained, let's play with it! What happens when we give it a prompt of something not in the dataset? Or can you craft the perfect prompt to generate an image you can imagine?\n",
    "\n",
    "The art of crafting a prompt to get the results you desire is called **prompt engineering**, and as shown here, is dependent on the kind of data the model is trained on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change me\n",
    "text_list = [\n",
    "    \"A daisy\",\n",
    "    \"A sunflower\",\n",
    "    \"A rose\"\n",
    "]\n",
    "\n",
    "model.eval()\n",
    "x_gen, x_gen_store = sample_flowers(text_list)\n",
    "grid = make_grid(x_gen.cpu(), nrow=len(text_list))\n",
    "other_utils.show_tensor_image([grid])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you've found a set of images you enjoy, run the below cell to turn it into an animation. It will be saved to [05_images/flowers.gif](05_images/flowers.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grids = [other_utils.to_image(make_grid(x_gen.cpu(), nrow=len(text_list))) for x_gen in x_gen_store]\n",
    "other_utils.save_animation(grids, \"05_images/flowers.gif\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations on making it to the end of the course! Hope the journey was enjoyable and you were able to generate something worthy of sharing with your friends and family.\n",
    "\n",
    "Ready to put your skills to the test?\n",
    "Head on over to the assessment to earn a certificate!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U8hHZEaPIo0A"
   },
   "source": [
    "<center><a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a></center>"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
